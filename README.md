# DATA603
Campus ID : LB04019
1) Big Data with example and types
Large, complicated sets of data that are challenging to manage and evaluate using conventional data processing techniques are referred to as "big data." As the internet expanded and digital technologies were used more often, the idea of "big data" began to take shape. This resulted in the production of enormous volumes of data from several sources.

Examples of big data include:

i) Social media data : Every day, social media sites like Facebook, Twitter, and Instagram produce enormous amounts of text, image, and video content. Consumer behavior, sentiment analysis, and social network analysis can all be done with the use of this data.

ii) Sensor data : Data gathered from wearables, industrial sensors, and smart home devices is referred to as sensor data. Predictive maintenance, smart city planning, and environmental monitoring can all make use of this data.

iii) Health care data : Data generated by the healthcare sector includes patient information, medical records, and information from clinical trials. Personalized medicine, drug research, and disease prediction can all benefit from this data.

Types of big data:

i) Structured Data : Data that has been organized and is easily processed by computers is said to have been structured. Databases, spreadsheets, and financial data are just a few examples of structured data.

ii) Unstructured Data : Unstructured data is difficult to process because it is not arranged in a precise format. Images, films, and audio recordings are a few examples of unstructured data.

iii) Semi-structured Data : Data that is partially structured combines both structured and unstructured data. Email messages, log files, and XML data are a few examples of semi-structured data.

Sources :

i) "What is Big Data?". IBM. https://www.ibm.com/analytics/hadoop/big-data-analytics

ii) "Types of Big Data: Structured, Semi-Structured and Unstructured Data". Simplilearn. https://www.simplilearn.com/types-of-big-data-article

2) 6 ‘V’s of Big Data (define each)

Volume, Velocity, Variety, Veracity, Value, and Visualization are the six Vs of big data. These traits are used to describe the potential and difficulties that huge and complex data collections bring.

i) Volume : The amount of data being created, saved, and evaluated is referred to as volume. The amount of data being generated is growing quickly as a result of the popularity of social media and IoT devices.

ii) Velocity : The rate at which data is produced and processed is referred to as velocity. For many firms, real-time data analysis is becoming more and more crucial, especially in fields like financial trading, fraud detection, and supply chain management.

iii) Variety : The term "variety" refers to the various sources and forms of data production, such as structured, unstructured, and semi-structured data. One of the main challenges organizations using big data must overcome is the complexity of managing a variety of data kinds and formats.

iv) Veracity : The quality and dependability of the data being created and evaluated are referred to as veracity. For businesses working with big data, one of the biggest challenges is how to deal with unconfirmed or inaccurate data, which is essential for making educated decisions.

v) Value : Value is the possible learnings and advantages that come from big data analysis. One of the main reasons why businesses engage in big data is the capacity to derive practical insights and make wise judgments.

vi) Variability : The data's irregularity and unpredictable nature. Due to variables including the seasonality of data, the variety of data sources, and the unpredictable nature of human behavior, big data can be extremely volatile. Variability management calls for adaptable and flexible data processing and analysis methods.

References:

i) Mayer-Schönberger, V., & Cukier, K. (2013). Big data: A revolution that will transform how we live, work, and think. Houghton Mifflin Harcourt.

ii) Manyika, J., Chui, M., Brown, B., Bughin, J., Dobbs, R., Roxburgh, C., & Byers, A. H. (2011). Big data: The next frontier for innovation, competition, and productivity. McKinsey Global Institute.

3) Phases of Big Data analysis (discuss each)

Phase 1 : Data Acquisition and Recording

* Data is produced from numerous sources.

- Telescopes, satellites, social media, scientific computers, and other Internet of Things (IoT) devices.

* A large portion of the data gathered is uninteresting.
- It can be magnitudes of times filtered and compressed.
- We must create effective filters that do not omit valuable information.
- Create the appropriate metadata automatically to define the types of data that are being measured and recorded.
 * Example: Most cameras record the date, time, and location of the photo when we snap one.
 
Phase 2 : Information Extraction and Cleaning
* Information extraction
- Extract the necessary data from the underlying sources and provide it in a structured manner that is appropriate for analysis
* Data cleaning
- Try to complete missing values, reduce noise while detecting outliers, and fix data discrepancies.

Phase 3: Data Integration, Aggregation, and Representation
* Data Integration and Aggregation
- Combine information from various (heterogeneous) data sources.
- Can aid in minimizing and avoiding duplications and inconsistencies

* Data Representation
- The same data can be represented in various ways in terms of data representation.
- Numerous methods of data visualization could be employed for representation, including: Pie chart, Boxplot, Histogram, and Bar chart

Phase 4: Query Processing, Data Modeling, and Analysis
* Basic data subsets can be obtained from the full collection of data using queries.
* An abstract model known as a data model standardizes how data items relate to one another and organizes data elements.
- Models for databases, entity relationships, semantic data models, etc.

* More than just SQL Querying is required for Big Data Analysis.
- In the SQL database, many data are not stored.
- It can be difficult to implement certain analysis tasks in SQL queries.

Phase 5: Interpretation
* Recognize and validate the output of the computer
Possibly in need of further information, such as the source of the data (provenance), the methodology used to arrive at each result, and the particular inputs used.
Visualization is beneficial.
ready to make a decision

References:

* Chen, M., Mao, S., & Liu, Y. (2014). Big data: A survey. Mobile Networks and Applications, 19(2), 171-209.
* Mariscal, G., Viveros, J., Ponce, J., & Alvarez, F. (2017). Big data processing: A survey. Journal of Big Data, 4(1), 24.
* Chen, Y., Mao, S., & Liu, Y. (2015). Big data: A survey of research at the intersection of computer science and economics. ACM Computing Surveys (CSUR), 47(4), 1-36.

4) Challenges in Big Data analysis (discuss each)

Challenge 1: Heterogeneity and Incompleteness

* Algorithms for machine analysis anticipate homogeneous data and are unable to comprehend subtlety or any other kind of minute deviations.
* More research is needed to effectively represent, access, and analyze semi-structured data.
* There may still be some incompleteness and inaccuracies in the data even after data cleaning and error repair.
- During the study of (probabilistic) data, this must be managed.

Challenge 2: Scale
* Data volume is increasing more quickly than computing capacity.
- Many predict that data will rise exponentially.
* Utilizing cloud computing is a viable alternative.
* Altering the storage subsystem could have an impact on all aspects of data processinge.
-  hard disk drive vs solid state drive; parallel file system 

Challenge 3: Timeliness
* The analysis's findings are frequently needed right away in numerous circumstances.
* Finding items in a huge data set that fit certain requirements is frequently important.
- Frequently, it is a query/search issue.
- Based on the characteristics of the dataset, new index structures are required.

Challenge 4: Privacy
* In the context of Big Data, worry over data privacy has increased.
- Big data can be used to infer new information based on location, time, and other data aspects.
* Effectively, managing privacy is a technological and a sociological issue.
* Data utility via data sharing vs. protecting data privacy

Challenge 5: Human Collaboration
* Analytics for Big Data should ideally combine both algorithmic and human analysis.
- Person in the loop
* A big data analysis system needs to accommodate input from various human experts as well as collaborative results exploration.
- We must be ready to handle disagreements, doubt, and mistakes caused by crowdsourcing platforms like Wikipedia, Yelp reviews, and Amazon Mechanical Turk.

